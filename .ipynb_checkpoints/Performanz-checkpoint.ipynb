{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What I need from you...\n",
    "\n",
    "Congratulations! You probably just opened the most interesting Jupyter Notebook ever.\n",
    "\n",
    "This Notebook is the result of many hours of hard work. \n",
    "And, I need something in return. From you. \n",
    "\n",
    "<video src=\"videos/Minions.mp4\" width=\"280\" height=\"157.5\" type=\"video/mp4\" controls>\n",
    "Your browser does not support the video tag\n",
    "</video> \n",
    "\n",
    "[[d]](#d) \n",
    " \n",
    "I created this Notebook as part of my bachelor thesis project. An evaluation of this Notebook is part of the thesis. This is why I need you to fill out a questionaire. \n",
    "\n",
    "**Duration to fill out questionaire:** A minimal evaluation takes 3 mouse clicks. In addition, if you wish to give a more detailed feetback, you can also answer the open questions. Any feedback will be very much appreciated. Answers can be in English or in German. \n",
    "\n",
    "<img src=\"figures/phone1.jpg\" width=\"500\"  alt=\"If 115 Students read this Notebook, I would need each one of them to fill out my questionaire at least 7.429 times in order to get significant results...\">\n",
    "\n",
    "Please fill out the form before the \\<date\\>. You can find the questionaire through this link: ...\n",
    "\n",
    "Thank you very much for your help! I hope the material presented here will help you deepen your understanding of performance in parallel computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I promise, I will fill out the questionaire before the due date ! \n",
      "\n",
      "confirm with yes or no: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nice, thank you so much!\n"
     ]
    }
   ],
   "source": [
    "%run topsecret/test1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Why are you looking at this notebook right now? I assume it is because you are learning about parallel programming and want to know all about it. But why is that so? Why are you bothering about learning the skill of parallel programming?\n",
    "\n",
    "<img src=\"figures/happyman.jpg\" width=\"400\"  alt=\"I always wanted to be able to say 'I once too took a course in parallel programming'...\">\n",
    "\n",
    "Well, probably you are aware of the importance of creating code with good performance, and you hope that parallel programming will help you with this. According to [[a]](#a) the main reason why we do parallel programming is to increase performance.\n",
    "\n",
    "But what is performance ? And to what extend can parallel programming help us increase performance ?\n",
    "\n",
    "When learning about parallel programming it is crucial to understand what performance is and how we can increase and measure it. \n",
    "\n",
    "In this Notebook you will learn about:\n",
    "\n",
    "- [Why we Need Parallel Programing](#why)\n",
    "- [Speed up](#speedup)\n",
    "- [Measuring Elapsed Time](#measuring)\n",
    "- [Calculating $t_{serial}$](#calculating)\n",
    "- [Overhead](#overhead)\n",
    "- [Amdahl's Law](#amdahl)\n",
    "- [Gustafson's Law](#gustafson)\n",
    "- Scalability\n",
    "- Efficiency\n",
    "- [References](#references)\n",
    "\n",
    "This notebook is not a more complicated version of a textbook, but a tool for interactive learning instead. You can write and execute your own code in code cells, and adjust the values of parameters I have defined in my code. You will not only learn about different laws of performance but will also be able to test the validity of these laws yourself. There are a number of benchmarks for this purpose, that you can find in the home-folder of this notebook. Of course you can also write your own serial and parallel programs and test their performance within this notebook. \n",
    "\n",
    "Have fun!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Parallel Programming ? <a class=\"anchor\" id=\"why\"></a>\n",
    "\n",
    "With the use of parallel programming we can not only speed up our programs, but we can also use the increased performance to reduce power consumption [[b]](#b):\n",
    "\n",
    "In this notebook we will not analyze the improvement of performance with regards to power consumption. Nevertheless, I recommend watching the following video by Tim Mattson [[c]](#c). He will talk about why we need parallel programming, and how parallel programming is influencing not only speed but also power consumption. This is the link: https://youtu.be/cMWGeJyrc9w.\n",
    "\n",
    "What did we learn from Tim Mattson in this video?\n",
    "\n",
    "We learned that with parallel computing we can either:\n",
    "1. Execute a program in the same amount of time, but with reduced power consumption\n",
    "2. or execute a program with the same amount of power consumption, but with reduced execution time\n",
    "\n",
    "In the following sections we will only evaluate performance with regards to execution time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed Up <a class=\"anchor\" id=\"speedup\"></a>\n",
    "\n",
    "The term 'speed up' is used to describe how much faster a new version of a program runs compared to the old version. This implies that we have 2 different execution times that we compare. In the case of parallel programming, it makes sense to compare \n",
    "- the execution time 'without parallelism' \n",
    "- the execution time 'with parallelism'\n",
    "\n",
    "Thus the speed up gives us a measurement for the improvement in speed that was achieved by applying parallelism to a program. \n",
    "\n",
    "We use the following formula to calculate speed up [[e]](#e):\n",
    "\n",
    "**Speed up = $\\frac{t_{serial}}{t_{parallel}}$**\n",
    "\n",
    "> $t_{serial}$ = execution time of the serial program, i.e. the program that does not use parallelism\n",
    "> <br>$t_{parallel}$ = execution time of the parallelized program \n",
    "\n",
    "**Question:**\n",
    "> Assume we have a serial program, that has an execution time of 54 seconds. Now we apply parallelism to our program. When we measure the execution time again, it is 27 seconds. \n",
    "\n",
    "> What speed up did we achieve ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My answer:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "That's right!\n",
      "Because 54/27=2 we have a speed up of two. This means, that the parallel version of the programm runs two times as fast than the serial one.\n"
     ]
    }
   ],
   "source": [
    "%run topsecret/test2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<img src='figures/race3.jpg' width='600' art='picture to illustrate speed up'>\n",
    "\n",
    "As you can see, calculating the speed up is pretty straight forward. There are, however, a few questions you need to ask yourself: \n",
    "\n",
    "   > [1. How can you measure the execution time ?](#1)<br>\n",
    "   > [2. Is it enough to measure the execution time just one time?](#2)<br>\n",
    "   > [3. How can you measure the execution time of a executable program from within Jupyter Notebook?](#3)<br>\n",
    "   > [4. Should you measure the execution time of the full pogramm run, or just a part of it?](#4)<br>\n",
    "\n",
    "In the following Section we will answer these 5 questions one by one.\n",
    "\n",
    "## Measuring Elapsed Time <a class=\"anchor\" id=\"measuring\"></a>\n",
    "\n",
    "### 1. How to measure execution time <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "In Jupyter Notebooks we can use the magic command `%%time` to measure the execution time of a code cell. Let's look at this example (the Code is in Python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 2 µs, total: 3 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = 2\n",
    "while a<100:\n",
    "    a *= 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we get CPU time and Wall time. Do you know the difference? \n",
    "\n",
    "`CPU time` measures the total time during which the CPU was busy executing our code, including time spent in library functions. The `wall time` on the other hand also measures the time, during which our program was idle. Wall time measures 'the time that has elapsed between the start and finish of execution of the code' [[a]](#a).\n",
    "\n",
    "**Question:** Which one do you think you need to measure, if you want to evaluate the performance of your parallel program?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write 'CPU Time' or 'Wall Time':\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " sda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You did't type a valid answer. Write 'CPU Time' or 'Wall Time'.\n"
     ]
    }
   ],
   "source": [
    "%run topsecret/test3.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** \n",
    "\"For example, in a distributed-memory program, a process that calls a receive function may have to wait for the sending process to execute the matching send, and the operating system might put the receiving process to sleep while it waits. This idle time wouldn't be counted as CPU time, since no function that's been called by the process is active. However, it should count in our evaluation of the overall run-time, since it may be a real cost in our program. If each time the program is run, the process has to wait, ignoring the time it spends waiting would give a misleading picture of the actual run-time of the program \" [[a]](#a).\n",
    "\n",
    "### 2. How many times do you have to run the code?<a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "**Challenge:** Run the code cell above (the one starting with %%time) multiple times. What do you observe ?\n",
    "\n",
    "You probably noticed that the wall time and CPU time differ each time you run the code. Since many other things are happening on the system while the code is executed, the exact values for CPU and wall time will differ each time we run the code. \n",
    "\n",
    "The following code will give you a graphical illustration of this effect. Note, that in the code below we measure the wall time without the help of the `%%time` magic command. This allow us to save the result of one single measurement multiple times, which we can then plot into a histogram. \n",
    "\n",
    "**Hint:** You can replace my code within the commented section with your own code if you want to graphically display the variance in wall time measurements. Should your code not be in Python, then you can make an executable of it instead, place the file in the same folder you have this notebook at, and replace my code with <br>`! ./nameOfYourBinary`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'micro seconds')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import timeit\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "seconds = 1\n",
    "miliSeconds = 1e3\n",
    "microSeconds = 1e6\n",
    "nanoSeconds = 1e9\n",
    "\n",
    "#start of constants you might want to adjust\n",
    "timeUnit = microSeconds #determines the time Unit on the x axis of the histogramm\n",
    "label = 'micro seconds' #this should be the same as the chosen timeUnit \n",
    "iterations = 100 #amount of measurements\n",
    "#end of constants you might want to adjust\n",
    "\n",
    "times = np.zeros(iterations)\n",
    "binsInHistogramm = iterations #amount of containers in the Histogramm\n",
    " \n",
    "\n",
    "for x in range(iterations):\n",
    "    time1 = timeit.default_timer()\n",
    "\n",
    "#start of the code you want to measure execution time on'''\n",
    "    a = 2\n",
    "    while a<100:\n",
    "        a *= 5\n",
    "#end of the code you want to measure execution time on'''\n",
    "\n",
    "    time2 = timeit.default_timer()\n",
    "    times[x]=(time2-time1)*timeUnit\n",
    "\n",
    "#display histogramm\n",
    "fig,ax = plt.subplots(figsize=(15,4))\n",
    "ax.hist(times, bins=binsInHistogramm)\n",
    "ax.set_title('wall time')\n",
    "ax.set_xlabel(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time you rerun the code, the histogram will look different. However, probably you notice one single value at the very right, each time you run the code. This makes it difficult to interpret the histogram values. \n",
    "\n",
    "**Question:** Do you think this is just random? Or could there be a good reason for this behavior?\n",
    "\n",
    "To answer this question, let's look at the first 20 values of our measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.30571425 0.62771142 0.54761767 0.52340329 0.48428774 0.46938658\n",
      " 0.46007335 0.46752393 0.48428774 0.461936   0.48056245 0.45262277\n",
      " 0.46566129 0.46938658 0.48056245 0.46566129 0.45076013 0.47311187\n",
      " 0.45634806 0.42282045]\n"
     ]
    }
   ],
   "source": [
    "if np.size(times)>=20:\n",
    "    print(times[:20])\n",
    "else: print(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the first value is much higher than the following ones. This makes sense, because the first time we run the code data has to be first loaded into the cache. \n",
    "\n",
    "Let's discard the first measurement and look at our histogram again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'micro seconds')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seconds = 1\n",
    "miliSeconds = 1000\n",
    "microSeconds = 1000000\n",
    "nanoSeconds = 1000000000\n",
    "\n",
    "#start of constants you might want to adjust\n",
    "timeUnit = microSeconds #determines the time Unit on the x axis of the histogramm\n",
    "label = 'micro seconds' #this should be the same as the chosen timeUnit \n",
    "iterations = 100 #amount of measurements\n",
    "#end of constants you might want to adjust\n",
    "\n",
    "times = np.zeros(iterations) \n",
    "binsInHistogramm = iterations #amount of containers in the Histogramm\n",
    "\n",
    "for x in range(iterations):\n",
    "    time1 = timeit.default_timer()\n",
    "\n",
    "#start of the code you want to measure execution time on'''\n",
    "    a = 2\n",
    "    while a<100:\n",
    "        a *= 5\n",
    "#end of the code you want to measure execution time on'''\n",
    "\n",
    "    time2 = timeit.default_timer()\n",
    "    times[x]=(time2-time1)*timeUnit\n",
    "\n",
    "#display histogramm\n",
    "fig,ax = plt.subplots(figsize=(15,4))\n",
    "ax.hist(times[1:], bins=binsInHistogramm)\n",
    "ax.set_title('wall time')\n",
    "ax.set_xlabel(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram looks much better for analysis. \n",
    "\n",
    "**Question**: Let's say you want to give one single value for the speed of your program/code section. Which value would you choose? The smallest? The biggest? An average? Would you include very large/small values when calculating the average? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which value would you choose? \n",
      "\n",
      "Type 'smallest','largest' or 'average': \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You did't type a valid answer.\n"
     ]
    }
   ],
   "source": [
    "%run topsecret/test4.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the number of iterations? How many times should you measure the wall time on your code, to avoid random noise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many iterations would you perform? \n",
      "\n",
      "Type a positive integer:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you just need an aproximate estimate, that will do the job. For a more exact result you probably want to increase the number of iterations.\n"
     ]
    }
   ],
   "source": [
    "%run topsecret/test5.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Let's try a few `number of iterations` and plot the results. For iteration number x we measure the wall time x times, and save the smallest measurement.\n",
    "\n",
    "**Hint:** You could replace my code within the commented section with your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = [10,100,1000,10000,100000,1000000,10000000]\n",
    "results = []\n",
    "\n",
    "for i in iterations:\n",
    "    result = np.zeros(i)\n",
    "    for x in range(i):\n",
    "        time1 = timeit.default_timer()\n",
    "\n",
    "    #start of the code you want to measure execution time on'''\n",
    "        a = 2\n",
    "        while a<100:\n",
    "            a *= 5\n",
    "    #end of the code you want to measure execution time on'''\n",
    "\n",
    "        time2 = timeit.default_timer()\n",
    "        result[x]=(time2-time1)*1000000 #micro-seconds\n",
    "    results.append(result.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f218d3b49d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot results\n",
    "fig,ax = plt.subplots(figsize=(10,5)) \n",
    "for i in range(len(iterations)):\n",
    "    ax.scatter(iterations[i], results[i], s=30, label=iterations[i])\n",
    "ax.set_xlabel('number of iterations (log scale)', fontsize=12)\n",
    "ax.set_ylabel('smallest execution time', fontsize=12)\n",
    "ax.set_title('Smallest Exectuion Time per Number of Iterations')\n",
    "ax.semilogx()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more often you run the code, the more likely you will get a measurement during which the system is not very busy doing other things. As we can see in the plot above, whether you run the code 10 times or 10'000 times can make a big difference. At some point though, the benefit you get from further increasing the number of iterations will become insignificant. When exactly that point is reached depends on: \n",
    "- the system\n",
    "- the background work happening on the system\n",
    "- the code \n",
    "\n",
    "When I run the code above on my system, it looks like after 1 million iterations there is no significant improvement for the minimal execution time. The python module timeit by default also makes 1 million measurements [[e]](#e) [[g]](#g), so this seems to be a good standard practice. For code that is more time consuming you might have to adjust the number of iterations for practical reasons.\n",
    "\n",
    "<img src='figures/thinking3.jpg' width='400' art='\"I could measure the execution time of my serial programm while doing a lot of things in the background. Then when I measure the execution time of my parallel programm, I could minimize the work happening in the background. This could significantly increase my speed-up...\"'>\n",
    "\n",
    "When you want to compare the execution times of different programs (such as a serial and a parallel version) it is obviously necessary, that you measure both execution times on the same system and with similar amount of background work happening on the system. There are multiple ways of how one could influence the measurements and thus distort the calculations of speed up. If you are interested in this topic, I recommend you read the following article \"Misleading Performance Reporting in the Supercomputing Field\" by David H. Bailey [[f]](#f).\n",
    "\n",
    "You might have noticed, that the wall time that we measured in the beginning with the magic command `%%time` differs from the time we measured manually with the use of `timeit.default_timer()`. \n",
    "\n",
    "### 3. How to measure execution time of a external program? <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "There are many different ways of measuring the wall time, and each methods will give slightly different results. The explanation for this is `overhead`. We will talk more about overhead later. Some functions will also by default make multiple measurements of the execution time, and output the acumulated time of all repeats. As long as you are consistent with the method you use for measuring time, it doesn't matter so much which one you choose.\n",
    "\n",
    "**Timeit Magic Command:** The probably most used method to measure execution time in Jupyter Notebooks is with the built-in magic command `timeit`. You might want to play around with the following options [[h]](#f):\n",
    "\n",
    "- -o allows you to save the `TimeitResult` object. This also gives you the chance to access some interesting attributes of the TimeitResult object \n",
    "- -q silences the output\n",
    "- -r sets the number of repeats\n",
    "- -n sets the number of loops for each repeat\n",
    "\n",
    "Maybe you feel confused about the difference between repeats and loops (I was). Just think of loops as 'repeats during each repeat'. In each repeat the code is executed n times (n=number set for loops), and the accumluated time measurement value of all loop runs is saved for every repeat. When you call the attribute `all_runs` you get the accumulated loop times for each 'repeat'. If you want to get the smallest non-accumulated time that was measured in all individual runs, you can use the attribute `best`.\n",
    "\n",
    "**Example:** To give an example, we are going to run a paralized sorting program written in c, and measure the total execution time with `%timeit`. You can find the code for this progam in `./extras/programs/quicksort`. The program is sorting an array of 10'000 random numbers with the quicksort algorithm, using 8 threads. We pass the number of threads to be used as the first argument, and the length of the array to be sorted as the second argument. The option `>/dev/null` silences the output of the program in the terminal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = %timeit -o -n 3 -r 9 -q ! ./programs/quicksort/notiming.o 64 10000 >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How many times did we run the program?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a number:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " seventy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorry, that is not correct. The option n tells us how many times the code is run during each repeat. The option r tells us how many repeats we have. Try again...\n"
     ]
    }
   ],
   "source": [
    "%run ./topsecret/test6.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Out of all these program runs, what is the value of the fastest run we measured? What code do you have to run to find out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type the code you would run, to find out the smallest execution time?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " time.best\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Very Good!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./topsecret/test7.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Do we want to measure the execution time of the entire program ? <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "In the method we used so far to measure the execution time of a program, we measured the time of the entire program run. Time consuming tasks that we might not necessarily want to include in our measurements are:\n",
    "- loading the program\n",
    "- loading runtime libraries\n",
    "- initializing data\n",
    "- allocating memory\n",
    "- testing if the calculated solution is correct\n",
    "- output of the result\n",
    "- ect.\n",
    "\n",
    "Be aware, that measuring execution time on the entire program is not standard practise [[a]](#a)! \n",
    "\n",
    "So how is it usually done? Well...different programmers do different things. Some will start measuring execution time right before the parallel region is entered and stop immedietly after the parallel region is left. Others will start timing when a function containing the main algorithm (e.g.Quicksort Alrgorithm) is entered, and stop timing upon return from this function. In both of these two scenarios data is usually initialized and memeory allocated before the timing phase begins, and any output or error testing will happen after the timing phase has stopped. \n",
    "\n",
    "In the folder `./programs` you can find various programs, that we will use to evaluate speed-up, efficiency, and laws of performance (Amdahl's law and Gustafson's Law). Each algorithm that you can find there has two different implementation versions. \n",
    "\n",
    "1. The parallel algorithm without any program internal time measuing \n",
    "2. The same parallel algorithm, but measuing the execution time spend during the parallel algorithm itself.\n",
    "\n",
    "With the second option we don't measure the time for loading the program and runtime libraries, as well as the initialization of data, output of the program, validation of the solution, etc. All the binaries of the first option are called `notiming.o`, whereas the binaries of the second option are called `timing.o`.\n",
    "\n",
    "Passed arguments for the `notiming.o` programms are: 1.Number of threads that should be used in the parallel regions, 2.Arguments for the Algorithm (e.g list length for soriting algoirhtms). Passed arguments for the `timing.o` programs are: 1. Number of threads, 2. Number of Repeats, 3. Arguments for the algorithm. The `timing.o` versions will save the smallest measured execution time in a text file called `time.txt`. The programs also work without passing any arguments, since there are default values set for all of them. \n",
    "\n",
    "Let's do a quick comparison of the execution times we get, depending on which program version we use. Here we have to make sure, that we use the same amount of repeats for each version. For the `notiming.o` version we set the number of repeats by setting the %timeit parameter `r` to the number of repeats we want. The `timing.o` versions take the number of repeats as the second argument when running it in the terminal.\n",
    "\n",
    "The graphics below display the difference of time measurment and speed up for the calculation of the Mandelbrot set. You can find the code for the graphics at `./extras/compare_timemeasuring_methods`. In this extra Notebook you can very easily create analog graphics for any algoirthm in the `./programs` section, even for your own programs if you add them (see `./readme.txt` file for details). \n",
    "\n",
    "<img src='figures/compare1.PNG' width='900' art='\"compare execution times of two different measuring methods.\"'>\n",
    "\n",
    "For both time mesuring options we see a simillilar trend: the more threads we have, the faster runs the program/algorithm. Further, the benefit from further increasing the number of threads seems to get smaller as the thread number increases. We will talk more about his in the section about Amdahl's law.\n",
    "\n",
    "If we don't pay much attention to the numbers on the y-axis we might think that we get more or less the same results. Be careful here, a more detailed analiyzis can change this first impression. \n",
    "\n",
    "Let's directly compare the absolute executions times:\n",
    "\n",
    "<img src='figures/compare2.PNG' width='450' art='\"compare execution times of two different measuring methods.\"'>\n",
    "\n",
    "It looks like the overhead we get from running the full program instead of only the algorithm is roughly the same for all thread numbers, with a slight increase as the thread number gets bigger. \n",
    "\n",
    "Since we are interessted in speed up and how different factors such as thread number and problem size (more about this later) influence speed-up, we should also look at the speed up calculations:\n",
    "\n",
    "<img src='figures/compare3.PNG' width='900' art='\"compare execution times of two different measuring methods.\"'>\n",
    "\n",
    "Well that is very interessting! On the two graphics further above we got the impression that we get more or less the same picture, regardless of which time measuring method we use. When we calcualte the speed up, however, we can see 2 things:\n",
    "1. The speed-ups we get, if we only measure the execution time on the algorithm, are much higher.\n",
    "2. The two curves have a different shape. On the left graph we can observe, that with the increased number of threads, the additional speed up gets smaller and smaller, and eventially plataoes. On the right graph we can also see a plateoing of the curve, this seems to be happening at a much higher number of threads however.\n",
    "\n",
    "Let's also directly compare the speed-ups:\n",
    "\n",
    "<img src='figures/compare4.PNG' width='450' art='\"compare execution times of two different measuring methods.\"'>\n",
    "\n",
    "We observe see very different calculations results for speed-up. This holds espcially true, as the number of thread increases. It seems surprising, that there could be such a big difference. This really shows, how easily we can influence the calculations of speed-up by adjusting how we measure time. It is thus important to have a critical mindset when reading about other researchers results regarding speed-up. \n",
    "\n",
    "Further we need to keep in mind, that we did all the calculations with only 10 repeats. As we have already found out earlier, this does not reduce random noise very much and thus gives us only approximate results (which is ok, if we are just exploring things and not writting a research paper...).\n",
    "\n",
    "**Possible add on** How does different number of repeats influence speed up...\n",
    "\n",
    "For the rest of this notebook we will measure only the elapsed time of the algorithm, and not the elapsed time of the entire program. This also allows us to increase the number of repeats (because it is faster) and thus get more precise results.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating $t_{serial}$  <a class=\"anchor\" id=\"calculating\"></a>\n",
    "\n",
    "Let's say we have some algorithm, for example the bubblesort algorithm for sorting a list of integers, and we wish to make it faster. What an excellent opportuniy to apply what we have learned about parallel programming. After parallelizing the selectionsort algorithm, we wish to examine the speed-up that we achieved. \n",
    "\n",
    "As a quick reminder, speed up is calculated like this:\n",
    "\n",
    "Speed up = $\\frac{t_{serial}}{t_{parallel}}$\n",
    "\n",
    "- To calculate $t_{parallel}$ we simply measure the time spent in the parallelized mergesort function\n",
    "\n",
    "- What about $t_{serial}$? You might think this is a question with an obvious answer: of course you would just take the serial version of your mergesort function, and measure the execution time. \n",
    "\n",
    "Let's have a look at the speed-up that we can achieve by calculating it this way. We are going to sort a list of 1'000 random numbers and set repeats to 100 and thread number set to 8 in the parallel program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/Thesis/programs/bubblesort\n",
      "\n",
      "Speed up of Bubblesort with 8 Threads:\n",
      "2.156883116883117\n"
     ]
    }
   ],
   "source": [
    "%cd ~/work/Thesis/programs/bubblesort\n",
    "! ./timing.o 8 100 1000 >/dev/null\n",
    "time_parallel_bubblesort = np.genfromtxt('./time.txt')\n",
    "! ./timing.o 1 100 1000 >/dev/null\n",
    "time_serial_bubblesort = np.genfromtxt('./time.txt')\n",
    "print(\"\\nSpeed up of Bubblesort with 8 Threads:\")\n",
    "print(time_serial_bubblesort/time_parallel_bubblesort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can't wait all day, we ran the code only 100 times. This means that the speed-up we end up calculating can vary quite a bit, if we run the code cell above multiple times. \n",
    "\n",
    "The speed up I got when I ran the cell was 2.15.\n",
    "\n",
    "So that's it, pretty easy and straight forward, right? \n",
    "\n",
    "**picture**teacher womean saying \"It looks easy, but you could make it much more complicated...\"\n",
    "\n",
    "One could argue, that $t_{serial}$ should be measured with the best possible algorithm for a specific problem. There are many sorting algorithms, and the fastest one for our specific list of numbers might not necessarily be selectonsort. Quicksort and Mergesort  are other sorting algorithms, which could be more efficient for aour specific problem. Let's compare the performance of these other sorting algorithms (serial version):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/Thesis/programs/bubblesort\n",
      "/home/jovyan/work/Thesis/programs/quicksort\n",
      "/home/jovyan/work/Thesis/programs/mergesort\n",
      "\n",
      "serial bubblesort: 0.004143\n",
      "serial quicksort: 8.2e-05\n",
      "serial mergesort: 0.000105\n"
     ]
    }
   ],
   "source": [
    "%cd ~/work/Thesis/programs/bubblesort\n",
    "! ./timing.o 1 100 1000 >/dev/null\n",
    "time_serial_selectionsort = np.genfromtxt('./time.txt')\n",
    "\n",
    "%cd ~/work/Thesis/programs/quicksort\n",
    "! ./timing.o 1 100 1000 >/dev/null\n",
    "time_serial_quicksort = np.genfromtxt('./time.txt')\n",
    "\n",
    "%cd ~/work/Thesis/programs/mergesort\n",
    "! ./timing.o 1 100 1000 >/dev/null\n",
    "time_serial_mergesort = np.genfromtxt('./time.txt')\n",
    "\n",
    "print('\\nserial bubblesort: '+ str(time_serial_bubblesort))\n",
    "print('serial quicksort: '+ str(time_serial_quicksort))\n",
    "print('serial mergesort: '+ str(time_serial_mergesort))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When sorting a list of 1'000 random numbers, we achieved much better performance with our serial quicksort and mergesort program, than with the serial bubblesort program. Would it not make more sense, to compare the parallel programm bubblesort with the most efficient serial sorting algorithm, which would be quicksort in our scenario? \n",
    "\n",
    "One might think this way, if the focus is on the problem of sorting 1'000 numbers. Were we to solve this problem on a serial machine, we would probably choose quicksort. So the imporvement we get by parallelizing bubblesort should be compared to the algorithm we would actually end up using on a serial machine. \n",
    "\n",
    "According to [[a]](#a) there are two different practices when measuring $t_{serial}$:\n",
    "1. take the best available serial algorithm (quicksort in our case)\n",
    "2. take the serial implementation of the algorithm you parallelized (bubblesort in our case)\n",
    "\n",
    "Let's have a look at the different speed-up's achieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed-up with Option 1:\n",
      "0.04385026737967915\n",
      "\n",
      "Speed-up with Option 2:\n",
      "2.2155080213903746\n"
     ]
    }
   ],
   "source": [
    "print(\"Speed-up with Option 1:\")\n",
    "print(time_serial_quicksort/time_parallel_bubblesort)\n",
    "print(\"\\nSpeed-up with Option 2:\")\n",
    "print(time_serial_bubblesort/time_parallel_bubblesort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on which option we choose, we get a different speed-up. In our scenario, we would be better off using the serial quicksort algorithm instead of the parallel bubblesort algorithm. When we just compare the bubblesort algoirhtm with itself (with and without parallelization) we get a speed up of +2.\n",
    "\n",
    "The option used most commonly is the second one [[a]](#a), which is also what we will be doing throughout the rest of this notebook. It is up to you to decide which option is best for your situaton. You must ensure, however, that other people know what serial algorithm you choose in your calculations of speed-up.\n",
    "\n",
    "If you looked at my code very alertly, you might have noticed, that I calculated the serial execution time by setting the number of threads to 1. I have in fact used the exact same exectuable than I used for measuring parallel execution time. Is this ok? Or would it be more accurate to write a sparate program for the measurement of serial execution time? In the folder `./extras` you can find a notebook `measuring_serial_time`, which analyses exactly that. Feel free to have a look, if you are interessted. The conclition basicaly is, that it is ok to just set thread number to 1, if you don't write a scientific paper and just want to explore things.\n",
    "\n",
    "In that extra Notebook you will also come across the term 'overhead'. I think it is time, to define what we mean with overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overhead <a class=\"anchor\" id=\"overhead\"></a>\n",
    "\n",
    "In the previous section we compared the execution time of parallel bubblesort to serial quicksort. We saw, that when we focus on the problem of sorting a list of 10'000 elements, it makes no sense to use bubblesort, because quicksort is so much faster. \n",
    "\n",
    "Do you wonder, if you could speed up things even more, by parallelizing quicksort? If serial quicksort was so much faster than parallel bubblesort, the parallel quicksort must be super super fast. Let's find out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/Thesis/programs/quicksort\n",
      "\n",
      "Speed up of Quicksort with 64 Threads:\n",
      "0.5364238410596026\n"
     ]
    }
   ],
   "source": [
    "%cd ~/work/Thesis/programs/quicksort\n",
    "! ./timing.o 64 100 1000 >/dev/null\n",
    "time_parallel_quicksort = np.genfromtxt('./time.txt')\n",
    "! ./timing.o 1 100 1000 >/dev/null\n",
    "time_serial_quicksort = np.genfromtxt('./time.txt')\n",
    "print(\"\\nSpeed up of Quicksort with 64 Threads:\")\n",
    "print(time_serial_quicksort/time_parallel_quicksort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that with a list length of 1000 and threads set to 64 we get a speed up smaller than 1. This means, that our serial version of the programm is faster than the parallel version. \n",
    "\n",
    "How can that be? Shouldn't parallelism make our programm run faster? The explanation for this is overhead. \n",
    "\n",
    "Creating parallel regions, loading the openmp runtime library, synchronization of threads, communication between threads, ... all this requires time. We don't need to spend any time on this in our serial version of the programm. The extra executation time due to parallelization we call 'paralleization overhead'. In most cases paralleization only makes sense for larger problem sizes [[a]]. \n",
    "\n",
    "Günther Bengel [[i]](#i) divides the total execution time of a programm into the following blocks:\n",
    "- $t_{cpu}$ : time used for calculations (using data in the local memory of the processors)\n",
    "- $t_{com}$ : time used for exchanging data inbeetween processors\n",
    "- $t_{wait}$ : waiting time (for example due to uneven loads on the different processors)\n",
    "- $t_{syn}$ : time used for synchronization of the used processors\n",
    "- $t_{place}$ : time used for allocating tasks to individual processors\n",
    "- $t_{start}$ : time used to stat the parallel tasks on all processors\n",
    "\n",
    "Günther Bengel [[i]](#i) further defines:\n",
    "- setup time = $t_{place} + t_{start}$\n",
    "- overhead = $t_{com} + t_{wait} + t_{syn}$\n",
    "\n",
    "Other authors define the setup time as part of the parallel overhead. According to [[a]](#a) parallel overhead is *'the part of the parallel run-time that’s due to any additional work that isn’t done by the serial program'*.\n",
    "\n",
    "There are many different kinds of overhead. In our case we have an overhead because we use Jupyter Notebook. For convenience we access the terminal through the Jupyter Notebook API, which results in overhead. Further we use the python module `timeit` for the time measurement, which again leads to a small overhead. And we have a operation system overhead as well [[j]](#j). The bigger the execution time of the programm, the more insiginificant these kinds of overheads become [[i]](#i). \n",
    "\n",
    "**Example:** Let's assume that the `OS`, `Jupyter Notebook API`, and the `pyhton module timeit` result in an overhead of 5 miliseconds. If the true execution time of our programm is also 5 miliseconds, then we would get a time measurement of 10 miliseconds with `timeit`. So we are 5 miliseconds off (because of the overhead).\n",
    "\n",
    "**Question:** Do you feel like this is error is relevant ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'yes or 'no':\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Very Good!!!That seems like a relevant error, as we end up with a measurement double as high than the true run time. \n"
     ]
    }
   ],
   "source": [
    "%run ./topsecret/test8.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What if we measure the performance of a larger program, that has a true runtime of 5 seconds? The time measurement we get with timeit would be 5 seconds and 5 milliseconds (because of overhead), so again we are 5 miliseconds off. Do yo feel like this error is relevant ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'yes or 'no':\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " kjoi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a valid answer.\n"
     ]
    }
   ],
   "source": [
    "%run ./topsecret/test9.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we learned that overhead matters. In fact, it can matter so much, that you might not even want to use your parallel program. In general we can say, that the bigger the problem size, the more likely it will be worth it using parallelization. In the case of sorting list length, the problem size can be increased by increasing the list length. \n",
    "\n",
    "Let's look again at the quicksort algorithm. This time we are going to use the parallel quicksort with 64 threads to sort different lengths of lists. We want to find out, how long our list has to be, so that it is worth it to use the parallel implementation with 64 threads. You can find the code for the graph below in the notebook `./extras/overhead`.\n",
    "\n",
    "This is my result:\n",
    "\n",
    "<img src='figures/overhead.PNG' width='700' art='When is it worth it using the parallel program instead of the serial one....\"'>\n",
    "\n",
    "We can see, that for all list length smaller than 1900 we get a speed-up smaller 1. Thus, it only makes sense for us to use the parallel implementaiton of quicksort, if our list is at least 1900 elements long. The reason for this is overhead.\n",
    "\n",
    "**Tipp:** It is a comman technique in parallel programming to set a threshold for when to use the serial implementation instead of the parallel one [[k]](#k). In our example we would set the threshold to 1900. If the list length is smaller than the threshold, a serial implementation of the algorithm can be used, otherwise a parallel one. This is especially useful in 'divide and conquer' algorithms, where the problem size often adjusted with each internal call of the function. To set a good value for the threshold you can do the kind of analyzis I did with the quicksort algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amdahl's Law <a class=\"anchor\" id=\"amdahl\"></a>\n",
    "\n",
    "Amdahl's Law allows us to calculate the maximum speed-up possible, for a specific program and a given number of processors. \n",
    "\n",
    "The question we can answer with the help of Amdahl's law is: \"With p processors, what is the maximum possible speed-up for this program?\"\n",
    "\n",
    "With the help of Amdahl's Law we can answer this question. However, we can only use Amdahl's Law if we know how many percent of the total program is inherently serial and which percentage of the program is parallelizable. \n",
    "\n",
    "We define:\n",
    "- $W_{ser}$ = total wall time of the serial part of the program\n",
    "- $W_{par}$ = total wall time of the parallelizable part of the program (executed with 1 thread)\n",
    "\n",
    "Let's give a small example with a story. Let's say we want to build a beach at our home. We rent one truck, and ask all our friends to help us bring the sand from the beach to our home. We tell half of our friends to hang out at the beach, the other half of our friends we ask to hang out at our house. With the help of our friends at the beach we shuffle as much sand into the truck as fits in there. Then we drive the truck to our home, and spread all the sand at the new 'home beach'. \n",
    "\n",
    "<img src='figures/beachstory.PNG' width='400' art='Story....\"'>\n",
    "\n",
    "Assuming we don't have any friends and have to do it all by ourselves, we need the following times:\n",
    "- filling the truck with sand: **6 hours**\n",
    "- driving the truck home: **2 hours**\n",
    "- spreading the sand at home: **3 hours**\n",
    "\n",
    "**Question:** What is $W_{ser}$ in this example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How many hours? \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nice, very good!\n"
     ]
    }
   ],
   "source": [
    "%run ./topsecret/test10.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is $W_{par}$ in this example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How many hours?: \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nice, very good!\n"
     ]
    }
   ],
   "source": [
    "%run ./topsecret/test11.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can share the work of filling the truck with sand and unloading the truck with our friends, so this work is parallelizable. However, regardless of how many friends we have, we won't be able to speed up the task of driving the truck back home. \n",
    "\n",
    "**Question:** With 3 friends at the beach and 3 friends at home, what is the minimal amount of hours we need for our project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How many hours?: \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Very good!\n"
     ]
    }
   ],
   "source": [
    "%run ./topsecret/test12.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we will need a little bit more time than this, because of overhead. Yet, we can give a lower bound for the execution time, which is:\n",
    "\n",
    "$t_{parallel} >= \\frac{W_{par}}{friends} + W_{ser}$\n",
    "\n",
    "Do you remember the question we ask with Amdahl's Law?  It is: \"With p processors, what is the maximum possible speed-up?\"\n",
    "\n",
    "Since $t_{serial}$ is 11 hours, and $t_{parallel}$ 5 or more, we get a speed up of maximum $\\frac{11}{5}$ = 2.2  if 3 friends come to help us at the beach and at home.\n",
    "\n",
    "**Question:** What if we are super popular, and we have an infinite number of friends? We will still need at least how many hours for our 'home beach' project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimal amount of hours needed: \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Very good! \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./topsecret/test13.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the maximum possible speed-up, if we have an infinite amount of friends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum possible speed-up: \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Try again...you need to divide t_serial (which is 11) by the smallest possible execution time.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./topsecret/test14.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. You just understood the basics of Amdahl's Law!!! <br>\n",
    "Yes, it really is that simple. Bascially what Amdahl's law says, is: \n",
    "\n",
    "<blockquote>\"No matter how many processors we can use for running our program in parallel, the maximum speed-up we can achieve is limited by $W_{ser}$ (the wall time of the part of the program, which cannot be parallelized).\" </blockquote> <br>\n",
    "\n",
    "<img src='figures/GeneAmdahl1960.jpg' width='400' art='Amdahl....\"'> [[i]](#i)\n",
    "\n",
    "<sub>Picture edited by Maruna Derieg.</sub>\n",
    "\n",
    "Let's go over Amdahl's Law again, but with a little bit more mathematics involved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the calculations for speed-up we used the notation $t_{serial}$ and $t_{parallel}$ for the execution times of the serial and parallel program run. From now onwards, we will write the execution time of the full program run as W (like wall time) and the number in the subscript describes the amount of processors used. Thus:\n",
    "\n",
    "- $W_{1} = t_{serial}$ \n",
    "- $W_{64} = t_{parallel}$ with 64 processors used\n",
    "\n",
    "Further we can say, that if we add up $W_{ser}$ and $W_{par}$ we get the wall time of the full program run (without using parallism).\n",
    "\n",
    ">$W_{1} = W_{ser} + W_{par}$  \n",
    "<sub>with $W_{1}$ = wall time of the total program run with number of processors set to 1\n",
    "\n",
    "If we increase the number of processors from 1 to 8, then the workload of the pararellizable part of the program can be shared amongst 8 threads. Thus $W_{par}$ would decrease to $W_{par}/8$ in the best case scenario. As we already know, there will be an overhead, so $W_{par}/8$ is just a lower bound. So we get:\n",
    "\n",
    ">$W_{8} >= W_{ser} + \\frac{W_{par}}{8}$  \n",
    "    \n",
    "And more generalized for any amount of processors:\n",
    ">$W_{p} >= W_{ser} + \\frac{W_{par}}{p}$  \n",
    "<sub>with p = number of processors\n",
    "    \n",
    "The formula for speed up with this new     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References <a class=\"anchor\" id=\"references\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[a] An introduction to parallel prgoramming. Peter Pachecco 2011.<a class=\"anchor\" id=\"a\"></a>\n",
    "\n",
    "[b] Introduction to Parallel Computing. From Algorithms to Programming on State-of-the-Art Platforms (2018) page 11? <a class=\"anchor\" id=\"b\"></a>\n",
    "\n",
    "[c] Time Mattson. Youtube Video \"Introduction to OpenMP: 02 part 1 Module 1\"  https://youtu.be/cMWGeJyrc9w <a class=\"anchor\" id=\"c\"></a>\n",
    "\n",
    "[d] Depicable Me. Youtube Video. \"Minions - WHAT ?!\" https://www.youtube.com/watch?v=MfylJy_nMbM <a class=\"anchor\" id=\"d\"></a>\n",
    "\n",
    "[e] Python Cookbook <a class=\"anchor\" id=\"e\"></a>\n",
    "\n",
    "[f] Misleading Performance Reporting in the Supercomputing Field, David H. Bailey <a class=\"anchor\" id=\"f\"></a>\n",
    "\n",
    "[g] Python Documentation. Timeit Module. https://docs.python.org/3/library/timeit.html <a class=\"anchor\" id=\"g\"></a>\n",
    "\n",
    "[h] Documentation about magic commands. https://ipython.readthedocs.io/en/stable/interactive/magics.html <a class=\"anchor\" id=\"h\"></a>\n",
    "\n",
    "[i] Masterkurs Parallele und verteilte Systeme.  <a class=\"anchor\" id=\"i\"></a>\n",
    "\n",
    "[j] introduction to parallel computing - from algorithm... <a class=\"anchor\" id=\"j\"></a>\n",
    "\n",
    "[k] patterns for parallel programming by tim mattson <a class=\"anchor\" id=\"k\"></a>\n",
    "\n",
    "[l] Picture of Gene Amdahl. https://cioperu.pe/fotoreportaje/15674/la-generalmente-interesante-historia-del-mainframe-de-ibm/?foto=5 <a class=\"anchor\" id=\"l\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
